{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683579623550,"user":{"displayName":"Joshua Gill","userId":"01256375150985738325"},"user_tz":-60},"id":"N0sX7qdG4KJF"},"outputs":[],"source":["import torch  \n","import torch.nn as nn \n","import torchvision\n","import torch.nn.functional as F\n","import torch.optim as optim \n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683578793089,"user":{"displayName":"Joshua Gill","userId":"01256375150985738325"},"user_tz":-60},"id":"Y8MwXelx4KJJ","outputId":"0a7173c6-58ab-47d0-a15e-a639e486cbe1"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["#Hyper parameters \n","Epoch_num = 100\n","batch_size =64\n","learning_rate = 0.01\n","\n","width = 224\n","height = 224\n","\n","device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1238,"status":"ok","timestamp":1683580053551,"user":{"displayName":"Joshua Gill","userId":"01256375150985738325"},"user_tz":-60},"id":"_kaLVZwK4KJK"},"outputs":[],"source":["training_transfrom = transforms.Compose([transforms.Resize((width,height)), transforms.RandomRotation(30),\n","                transforms.RandomVerticalFlip(),transforms.RandomHorizontalFlip(), \n","                transforms.ToTensor() ,transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n","\n","training_data =  datasets.Flowers102(root='data', split='train', download= True, transform= training_transfrom)\n","\n","training_loader = DataLoader(training_data, batch_size=batch_size, shuffle= True, num_workers=2)\n","\n","val_transform = transforms.Compose([transforms.Resize((width,height)),transforms.ToTensor(), \n","                                    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n","val_data = datasets.Flowers102(root='data', split = 'val', download=True, transform = val_transform)\n","\n","val_loader = DataLoader(val_data, batch_size= batch_size, shuffle=True)\n","\n","test_transform = transforms.Compose([transforms.Resize((width,height)), transforms.ToTensor(), \n","                                     transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])\n","\n","test_data = datasets.Flowers102(root='data', split = 'test', download = True, transform = test_transform)\n","test_loader = DataLoader(test_data, batch_size= batch_size, shuffle = True)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683578817014,"user":{"displayName":"Joshua Gill","userId":"01256375150985738325"},"user_tz":-60},"id":"Ni8yaJyr4KJL","outputId":"72b17996-3b71-4528-f9b4-707d31238d9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Adjusting learning rate of group 0 to 1.0000e-02.\n","CNN(\n","  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n","  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n","  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=93312, out_features=2048, bias=True)\n","  (fc2): Linear(in_features=2048, out_features=2048, bias=True)\n","  (fc3): Linear(in_features=2048, out_features=102, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (dropout2): Dropout(p=0.5, inplace=False)\n",")\n"]}],"source":["class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(5,5), stride = (1,1), padding=(1,1))\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5,5), stride = (1,1), padding=(1,1))\n","        self.conv3 = nn.Conv2d(in_channels=64, out_channels= 128, kernel_size= (3,3), stride=(1,1), padding=(1,1))\n","        self.conv4 = nn.Conv2d(in_channels = 128, out_channels= 128, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n","        self.pool = nn.MaxPool2d(kernel_size= (2,2), stride= (2,2))\n","        self.fc1 = nn.Linear(128*27*27, 2048)\n","        self.fc2 = nn.Linear(2048, 2048)\n","        self.fc3 = nn.Linear(2048, 102)\n","        self.dropout = nn.Dropout(0.2)\n","        self.dropout2 = nn.Dropout(0.5)\n","        \n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x)\n","        # print(x.shape)\n","        # x = self.dropout(x)\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        # print(x.shape)\n","        x = F.relu(self.conv3(x))\n","        # x = self.pool(x)\n","        # print(x.shape)\n","        x = F.relu(self.conv4(x))\n","        x = self.pool(x)\n","        # print(x.shape)\n","        x = x.view(-1, 128 *27 *27)\n","        # x = x.view(x.size(0), -1)\n","        x = self.dropout(self.fc1(x))\n","        x = self.dropout(self.fc2(x))\n","        # x = self.fc2(x)\n","        x = self.fc3(x)\n","        return x\n","        \n","\n","\n","model = CNN().to(device)\n","\n","optimiser = optim.SGD(model.parameters(),lr=learning_rate, momentum=0.95)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","scheduler = optim.lr_scheduler.StepLR(optimiser, step_size= 30, gamma=0.5, verbose= True)\n","\n","# x = torch.randn(0, 3, 240, 240)\n","# y = model(x)\n","# print(y.shape)\n","print(model)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([64, 128, 27, 27])\n"]}],"source":["image, label = next(iter(training_loader))\n","\n","image = image.to(device)\n","\n","x = F.relu(model.conv1(image))\n","x = model.pool(x)\n","# print(x.shape)\n","x = F.relu(model.conv2(x))\n","x = model.pool(x)\n","# print(x.shape)\n","x = F.relu(model.conv3(x))\n","# x = model.pool(x)\n","# print(x.shape)\n","x = F.relu(model.conv4(x))\n","x = model.pool(x)\n","print(x.shape)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":806536,"status":"ok","timestamp":1683579623549,"user":{"displayName":"Joshua Gill","userId":"01256375150985738325"},"user_tz":-60},"id":"ieKKSOIC4KJM","outputId":"bcda5faa-0ce9-4389-efcd-5f8cecc60150"},"outputs":[{"name":"stdout","output_type":"stream","text":["Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 0 \tTraining Loss: 4.627452 \tValidation Loss: 4.619585 \tAccuracy: 1.470588\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 1 \tTraining Loss: 4.611134 \tValidation Loss: 4.573976 \tAccuracy: 2.941176\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 2 \tTraining Loss: 4.433834 \tValidation Loss: 4.212840 \tAccuracy: 3.627451\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 3 \tTraining Loss: 4.226733 \tValidation Loss: 4.068200 \tAccuracy: 6.568627\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 4 \tTraining Loss: 4.001328 \tValidation Loss: 3.897838 \tAccuracy: 10.000000\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 5 \tTraining Loss: 3.914118 \tValidation Loss: 4.135549 \tAccuracy: 5.784314\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 6 \tTraining Loss: 3.805372 \tValidation Loss: 3.992198 \tAccuracy: 7.156863\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 7 \tTraining Loss: 3.745871 \tValidation Loss: 3.856111 \tAccuracy: 7.549020\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 8 \tTraining Loss: 3.561318 \tValidation Loss: 3.717734 \tAccuracy: 11.666667\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 9 \tTraining Loss: 3.460747 \tValidation Loss: 3.629409 \tAccuracy: 14.803922\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 10 \tTraining Loss: 3.455817 \tValidation Loss: 3.650784 \tAccuracy: 11.960784\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 11 \tTraining Loss: 3.379991 \tValidation Loss: 3.743640 \tAccuracy: 15.000000\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 12 \tTraining Loss: 3.228967 \tValidation Loss: 3.558075 \tAccuracy: 17.352941\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 13 \tTraining Loss: 3.107499 \tValidation Loss: 3.904829 \tAccuracy: 15.490196\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 14 \tTraining Loss: 3.134122 \tValidation Loss: 3.755958 \tAccuracy: 14.313725\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 15 \tTraining Loss: 3.000218 \tValidation Loss: 3.813414 \tAccuracy: 16.470588\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 16 \tTraining Loss: 2.864129 \tValidation Loss: 3.674502 \tAccuracy: 15.098039\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 17 \tTraining Loss: 2.863170 \tValidation Loss: 3.547585 \tAccuracy: 17.549020\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 18 \tTraining Loss: 3.108039 \tValidation Loss: 4.281014 \tAccuracy: 12.156863\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 19 \tTraining Loss: 2.922706 \tValidation Loss: 3.759807 \tAccuracy: 14.313725\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 20 \tTraining Loss: 2.923842 \tValidation Loss: 3.765544 \tAccuracy: 16.666667\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 21 \tTraining Loss: 2.763189 \tValidation Loss: 3.965926 \tAccuracy: 13.529412\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 22 \tTraining Loss: 2.870092 \tValidation Loss: 3.797505 \tAccuracy: 15.392157\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 23 \tTraining Loss: 2.945464 \tValidation Loss: 4.234376 \tAccuracy: 13.333333\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 24 \tTraining Loss: 2.654832 \tValidation Loss: 3.899244 \tAccuracy: 15.294118\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 25 \tTraining Loss: 2.455097 \tValidation Loss: 3.937050 \tAccuracy: 16.960784\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 26 \tTraining Loss: 2.432901 \tValidation Loss: 3.685468 \tAccuracy: 19.019608\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 27 \tTraining Loss: 2.427629 \tValidation Loss: 3.650587 \tAccuracy: 15.490196\n","Adjusting learning rate of group 0 to 1.0000e-02.\n","Epoch: 28 \tTraining Loss: 2.404600 \tValidation Loss: 4.054806 \tAccuracy: 19.117647\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 29 \tTraining Loss: 2.240544 \tValidation Loss: 3.820637 \tAccuracy: 18.137255\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 30 \tTraining Loss: 1.890595 \tValidation Loss: 3.961029 \tAccuracy: 20.098039\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 31 \tTraining Loss: 1.809360 \tValidation Loss: 3.936617 \tAccuracy: 20.882353\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 32 \tTraining Loss: 1.636044 \tValidation Loss: 4.513095 \tAccuracy: 20.686275\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 33 \tTraining Loss: 1.561222 \tValidation Loss: 4.161083 \tAccuracy: 21.274510\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 34 \tTraining Loss: 1.504087 \tValidation Loss: 3.919589 \tAccuracy: 21.764706\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 35 \tTraining Loss: 1.333151 \tValidation Loss: 4.344822 \tAccuracy: 22.647059\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 36 \tTraining Loss: 1.405625 \tValidation Loss: 4.007451 \tAccuracy: 22.549020\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 37 \tTraining Loss: 1.348647 \tValidation Loss: 4.367016 \tAccuracy: 22.843137\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 38 \tTraining Loss: 1.269584 \tValidation Loss: 4.224632 \tAccuracy: 22.352941\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 39 \tTraining Loss: 1.278403 \tValidation Loss: 4.712733 \tAccuracy: 21.176471\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 40 \tTraining Loss: 1.241180 \tValidation Loss: 4.500134 \tAccuracy: 20.294118\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 41 \tTraining Loss: 1.216913 \tValidation Loss: 4.746219 \tAccuracy: 23.137255\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 42 \tTraining Loss: 1.044522 \tValidation Loss: 4.547282 \tAccuracy: 24.607843\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 43 \tTraining Loss: 1.070642 \tValidation Loss: 4.529396 \tAccuracy: 23.529412\n","Adjusting learning rate of group 0 to 5.0000e-03.\n","Epoch: 44 \tTraining Loss: 0.936564 \tValidation Loss: 4.847573 \tAccuracy: 24.117647\n"]}],"source":["train_losses = []\n","valid_lossess = []\n","\n","for epoch in range(Epoch_num):\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    correct = 0\n","    total = 0\n","    best_accuracy = 38\n","\n","    \n","    model.train()\n","    for batch, (data,target) in enumerate(training_loader):\n","        \n","        data = data.to(device)\n","        target = target.to(device)\n","        \n","        optimiser.zero_grad()\n","        out = model(data)\n","        loss = criterion(out, target)\n","        loss.backward()\n","        optimiser.step()\n","        train_loss += loss.item() * data.size(0)\n","        \n","        \n","    model.eval()\n","    with torch.no_grad():\n","      for data, target in val_loader:\n","        \n","          data = data.to(device)\n","          target = target.to(device)\n","        \n","          out = model(data)\n","\n","          _, predicted = torch.max(out.data, 1)\n","\n","          total += target.size(0)\n","          correct += (predicted == target).sum().item()\n","          loss = criterion(out, target)\n","\n","          valid_loss += loss.item() * data.size(0)\n","    accuracy = (correct / total) *100\n","    \n","    if(accuracy > best_accuracy):\n","      best_accuracy = accuracy\n","      torch.save(model, 'model_best')\n","    scheduler.step()\n","    train_loss = train_loss/len(training_loader.sampler)\n","    valid_loss = valid_loss/len(val_loader.sampler)\n","    train_losses.append(train_loss)\n","    valid_lossess.append(valid_loss)\n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tAccuracy: {:.6f}'.format(epoch, train_loss, valid_loss, accuracy))\n","    \n","\n","def final_test(test_loader, model):\n","  with torch.no_grad():\n","    correct = 0 \n","    total = 0\n","    for data, target in test_loader:\n","      data = data.to(device)\n","      target = target.to(device)\n","\n","      out = model(data)\n","\n","      _, predicted = torch.max(out.data, 1)\n","\n","      total += target.size(0)\n","\n","      correct +=( predicted == target).sum().item()\n","\n","    accuracy = (correct / total) *100\n","    print(\"Accuracy: {:.2f}\".format(accuracy))\n","\n","best_model = torch.load('model_best')\n","final_test(test_loader, best_model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_model = torch.load('model_best')\n","final_test(test_loader,best_model)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1CdIkXWw5WzdiEDeLyqW84GJ1Nat2Zauc","timestamp":1683580280229}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
